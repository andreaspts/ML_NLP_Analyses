{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple applications of NLTK\n",
    "\n",
    "In the following notebook, we exemplify some applications of the NLTK (Natural Language Tool KIT) library for NLP. For an overview of important terminology in NLP, we refer to [here](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html). For a nice overview of applications of NLTK in python, consider this [link](https://likegeeks.com/nlp-tutorial-using-python-nltk/). To get a broader perspective on other libraries for NLP in python, go [here](https://medium.com/@srimanikantapalakollu/top-5-natural-language-processing-python-libraries-for-data-scientist-32463d36feae).\n",
    "\n",
    "### The points we focus on here are:\n",
    "\n",
    "- Desintegration of a text into tokens (sentences and words)\n",
    "- Stemming (a way to delete endings of words to establish correlations in between word stems)\n",
    "- POS-Tagging (Part of Speech) (to identify the type of word: noun, verb, adjective etc.)\n",
    "- relevance of words/sentiment analysis\n",
    "\n",
    "For this, we use data originally retrieved from [here](https://www.kaggle.com/snap/amazon-fine-food-reviews). In the respository a processed version of it is provided. For a gentle (and more detailed) introduction of the first part (tokenizing), check also [here](https://github.com/andreaspts/ML_NLP_Analyses/blob/master/simple_exploration_into_regular_expressions.ipynb). For the last part, we use the information gained before to train a (multinomial) naive Bayes model. (The latter was explored [here](https://github.com/andreaspts/ML_Naive_Bayes_for_Classification).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He went into a supermarket in St. Louis.', 'There, he bought wine for 10$.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization via a trained machine learning model\n",
    "\n",
    "text = \"He went into a supermarket in St. Louis. There, he bought wine for 10$.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer (a machine learning model) is smart enough to split at a full stop an not at the . after St!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'went', 'into', 'a', 'supermarket', 'in', 'St.', 'Louis', '.']\n",
      "['There', ',', 'he', 'bought', 'wine', 'for', '10', '$', '.']\n"
     ]
    }
   ],
   "source": [
    "#splitting the sentence is done via the regular expression method\n",
    "for sentence in sentences:\n",
    "    print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) POS-Tagging\n",
    "\n",
    "POS tagging can be done in various ways (i.e. via different machine learning methods) via the NLTK library. A nice overview with performance check can be found here: https://natemccoy.github.io/2016/10/27/evaluatingnltktaggerstutorial.html.\n",
    "\n",
    "The following discussion is baded on the perceptron tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('a', 'DT'), ('supermarket', 'NN'), ('in', 'IN'), ('St.', 'NNP'), ('Louis', 'NNP'), ('.', '.')]\n",
      "[('There', 'EX'), (',', ','), ('he', 'PRP'), ('bought', 'VBD'), ('wine', 'NN'), ('for', 'IN'), ('10', 'CD'), ('$', '$'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#to assign a tag (what type) to the tokens\n",
    "for sentence in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags for each object are called treebank tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He/PRP went/VBD into/IN a/DT supermarket/NN in/IN St./NNP Louis/NNP ./.\n",
      "There/EX ,/, he/PRP bought/VBD wine/NN for/IN 10/CD $/$ ./.\n"
     ]
    }
   ],
   "source": [
    "#develop model which filters for adjectives\n",
    "\n",
    "for sentence in sentences:\n",
    "    tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    final_sentence = []\n",
    "    for tagged_word in tagged_words:\n",
    "        final_sentence.append(tagged_word[0] + \"/\" + tagged_word[1])\n",
    "        \n",
    "    #print(final_sentence)\n",
    "    print(\" \".join(final_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tagged sentences could be further processed using machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Stemming\n",
    "\n",
    "An overview of stemming algorithms for various European languages can be found here https://snowballstem.org/algorithms/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant packages\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quick'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'follow'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"followed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Lemmatizing\n",
    "\n",
    "Lemmatizer work in a similar fashion as stemmers but are able to retrieve word associations (at the cost of computational time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "l = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lemmatize(\"going\", \"v\") #associate to verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He is going to the supermarket down the street.\"\n",
    "\n",
    "words_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #if we can't identify the type, we return that it is a noun (crude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "be\n",
      "go\n",
      "to\n",
      "the\n",
      "supermarket\n",
      "down\n",
      "the\n",
      "street\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in words_tagged:\n",
    "    \n",
    "    print(l.lemmatize(word[0], get_wordnet_pos(word[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Application: Combination of NLTK and machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId  HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
       "0   1  B001E4KFG0                     1                       1      5   \n",
       "1   2  B00813GRG4                     0                       0      1   \n",
       "2   3  B000LQOCH0                     1                       1      4   \n",
       "3   4  B000UA0QIQ                     3                       3      2   \n",
       "4   5  B006K2ZZ7K                     0                       0      5   \n",
       "\n",
       "         Time                Summary  \\\n",
       "0  1303862400  Good Quality Dog Food   \n",
       "1  1346976000      Not as Advertised   \n",
       "2  1219017600  \"Delight\" says it all   \n",
       "3  1307923200         Cough Medicine   \n",
       "4  1350777600            Great taffy   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define data frame\n",
    "\n",
    "df = pd.read_csv(\"./Reviews_10000.csv.bz2\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      "Id                        10000 non-null int64\n",
      "ProductId                 10000 non-null object\n",
      "HelpfulnessNumerator      10000 non-null int64\n",
      "HelpfulnessDenominator    10000 non-null int64\n",
      "Score                     10000 non-null int64\n",
      "Time                      10000 non-null int64\n",
      "Summary                   10000 non-null object\n",
      "Text                      10000 non-null object\n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable/text column with descriptions\n",
    "df = df#.sample(100)\n",
    "texts = df[\"Text\"]#.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting text into sentences\n",
    "\n",
    "texts_transformed = []\n",
    "\n",
    "for review in texts:\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    adjectives = []\n",
    "    #tokenizing sentences into words\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        #proceed with pos-tagging\n",
    "        words_tagged = nltk.pos_tag(words)\n",
    "        for word_tagged in words_tagged:\n",
    "            #filter for adjectives\n",
    "            if word_tagged[1] == \"JJ\":\n",
    "                adjectives.append(word_tagged[0])\n",
    "    texts_transformed.append(\" \".join(adjectives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(texts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for training\n",
    "\n",
    "X = texts_transformed\n",
    "Y = (df[\"Score\"] >=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words and their occurrence have to be transformed into integers\n",
    "cv = CountVectorizer(max_features = 100) # take the max_features most often appearing words\n",
    "cv.fit(X_train)\n",
    "\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 100)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.854125\n",
      "Test score: 0.8455\n"
     ]
    }
   ],
   "source": [
    "#define Naive Bayes ML model: MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Training score: \" + str(model.score(X_train, Y_train)))\n",
    "print(\"Test score: \"  + str(model.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'added',\n",
       " 'amazing',\n",
       " 'artificial',\n",
       " 'available',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'big',\n",
       " 'bitter',\n",
       " 'black',\n",
       " 'bold',\n",
       " 'br',\n",
       " 'brown',\n",
       " 'cheap',\n",
       " 'cold',\n",
       " 'dark',\n",
       " 'delicious',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'disappointed',\n",
       " 'dry',\n",
       " 'due',\n",
       " 'easy',\n",
       " 'excellent',\n",
       " 'expensive',\n",
       " 'extra',\n",
       " 'fantastic',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'few',\n",
       " 'fine',\n",
       " 'first',\n",
       " 'flavored',\n",
       " 'flavorful',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'full',\n",
       " 'glad',\n",
       " 'gluten',\n",
       " 'good',\n",
       " 'great',\n",
       " 'green',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'healthy',\n",
       " 'high',\n",
       " 'hot',\n",
       " 'huge',\n",
       " 'large',\n",
       " 'last',\n",
       " 'light',\n",
       " 'little',\n",
       " 'local',\n",
       " 'long',\n",
       " 'low',\n",
       " 'many',\n",
       " 'mild',\n",
       " 'much',\n",
       " 'natural',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'old',\n",
       " 'only',\n",
       " 'organic',\n",
       " 'original',\n",
       " 'other',\n",
       " 'own',\n",
       " 'perfect',\n",
       " 'picky',\n",
       " 'pleased',\n",
       " 'quick',\n",
       " 'real',\n",
       " 'regular',\n",
       " 'rich',\n",
       " 'right',\n",
       " 'same',\n",
       " 'second',\n",
       " 'several',\n",
       " 'similar',\n",
       " 'single',\n",
       " 'small',\n",
       " 'smooth',\n",
       " 'soft',\n",
       " 'sour',\n",
       " 'special',\n",
       " 'strong',\n",
       " 'such',\n",
       " 'super',\n",
       " 'sure',\n",
       " 'sweet',\n",
       " 'tasty',\n",
       " 'top',\n",
       " 'weak',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wonderful',\n",
       " 'worth',\n",
       " 'wrong']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the names of the max_features are given and its order matters for the naive Bayes approach\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.02042506, -5.6974861 , -5.51339306, -5.50458243, -4.91290471,\n",
       "       -5.64566103, -4.60242621, -4.72735213, -5.06463115, -5.3135272 ,\n",
       "       -5.23014559, -4.76014196, -5.80996408, -6.09764615, -5.76344407,\n",
       "       -5.25034829, -4.00441129, -4.29798151, -5.88407205, -6.25179683,\n",
       "       -5.46165739, -5.68690399, -4.23689381, -4.5204691 , -4.75598395,\n",
       "       -5.01503421, -5.59638998, -5.7189923 , -4.09017811, -4.21009562,\n",
       "       -5.25717426, -4.03622312, -5.36558356, -5.32079995, -4.02817478,\n",
       "       -5.56795205, -4.40013922, -4.78545977, -5.3135272 , -5.10498244,\n",
       "       -2.44719408, -2.71892163, -4.89844663, -4.53367062, -4.57435419,\n",
       "       -4.43564591, -4.4538835 , -3.97358033, -5.84633173, -5.0932864 ,\n",
       "       -4.69560344, -4.9275749 , -3.38011721, -4.54368739, -4.97292272,\n",
       "       -4.76431733, -3.988877  , -5.77487276, -3.84719013, -4.50420858,\n",
       "       -4.83360911, -5.15318455, -4.03420495, -4.40013922, -4.46312856,\n",
       "       -4.55042143, -5.7189923 , -3.12926876, -4.71931996, -4.35467685,\n",
       "       -5.76344407, -5.54030051, -5.48719069, -4.82024588, -4.32188703,\n",
       "       -5.03677419, -4.90806209, -4.49138789, -5.50458243, -4.75184315,\n",
       "       -5.63561069, -5.48719069, -4.29798151, -4.94246351, -5.29202099,\n",
       "       -5.88407205, -5.79812962, -4.21491491, -5.3355061 , -5.67643269,\n",
       "       -4.56402708, -4.20052617, -4.53035386, -5.83406163, -5.64566103,\n",
       "       -5.21034296, -4.63866206, -4.55380552, -5.85875425, -5.76344407])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the parameters of the MultinomialNB are given\n",
    "model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we assemble these together to get a list of tuples\n",
    "adj = list(zip(model.coef_[0], cv.get_feature_names()))\n",
    "\n",
    "#sort this list according to coefficients in the tuples\n",
    "adj = sorted(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-6.251796833976684, 'disappointed')\n",
      "(-6.097646154149426, 'cheap')\n",
      "(-5.884072053851367, 'difficult')\n",
      "(-5.884072053851367, 'sour')\n",
      "(-5.858754245867077, 'worth')\n",
      "(-5.846331725868519, 'huge')\n",
      "(-5.834061633276705, 'top')\n",
      "(-5.809964081697645, 'brown')\n",
      "(-5.798129624050642, 'special')\n",
      "(-5.774872761886375, 'mild')\n",
      "(-5.7634440660627515, 'cold')\n",
      "(-5.7634440660627515, 'picky')\n",
      "(-5.7634440660627515, 'wrong')\n",
      "(-5.718992303491918, 'fat')\n",
      "(-5.718992303491918, 'original')\n",
      "(-5.6974860982709545, 'added')\n",
      "(-5.686903988940418, 'due')\n",
      "(-5.676432689073122, 'super')\n",
      "(-5.645661030406369, 'awesome')\n",
      "(-5.645661030406369, 'weak')\n",
      "(-5.635610694552867, 'similar')\n",
      "(-5.596389981399586, 'fantastic')\n",
      "(-5.567952046079053, 'french')\n",
      "(-5.540300514748542, 'pleased')\n",
      "(-5.5133930618286175, 'amazing')\n",
      "(-5.504582432146463, 'artificial')\n",
      "(-5.504582432146463, 'second')\n",
      "(-5.487190689434594, 'quick')\n",
      "(-5.487190689434594, 'single')\n",
      "(-5.461657387429429, 'dry')\n",
      "(-5.365583557339807, 'flavored')\n",
      "(-5.335506102102529, 'such')\n",
      "(-5.320799954712833, 'flavorful')\n",
      "(-5.313527195383753, 'black')\n",
      "(-5.313527195383753, 'glad')\n",
      "(-5.29202099016279, 'soft')\n",
      "(-5.257174258832622, 'fine')\n",
      "(-5.250348293762222, 'dark')\n",
      "(-5.230145586444703, 'bold')\n",
      "(-5.210342959148523, 'white')\n",
      "(-5.153184545308575, 'next')\n",
      "(-5.104982443490696, 'gluten')\n",
      "(-5.093286403727506, 'large')\n",
      "(-5.0646311479671295, 'bitter')\n",
      "(-5.036774193464163, 'rich')\n",
      "(-5.020425055462634, 'able')\n",
      "(-5.015034206827757, 'extra')\n",
      "(-4.97292272147763, 'long')\n",
      "(-4.942463513992922, 'smooth')\n",
      "(-4.927574901499171, 'light')\n",
      "(-4.912904711751377, 'available')\n",
      "(-4.90806208727559, 'right')\n",
      "(-4.898446628576147, 'green')\n",
      "(-4.8336091090808395, 'new')\n",
      "(-4.820245881268672, 'real')\n",
      "(-4.785459765183257, 'full')\n",
      "(-4.764317328609447, 'low')\n",
      "(-4.760141957198967, 'br')\n",
      "(-4.755983947050304, 'expensive')\n",
      "(-4.751843154384272, 'several')\n",
      "(-4.727352134375976, 'big')\n",
      "(-4.719319962678712, 'own')\n",
      "(-4.695603436061396, 'last')\n",
      "(-4.638662059661257, 'whole')\n",
      "(-4.602426211207213, 'bad')\n",
      "(-4.574354192428526, 'hard')\n",
      "(-4.564027078272677, 'sure')\n",
      "(-4.553805524201138, 'wonderful')\n",
      "(-4.550421426216898, 'organic')\n",
      "(-4.543687394035554, 'local')\n",
      "(-4.533670615792083, 'happy')\n",
      "(-4.530353863166089, 'tasty')\n",
      "(-4.5204691039335465, 'excellent')\n",
      "(-4.504208583061766, 'natural')\n",
      "(-4.491387894632705, 'same')\n",
      "(-4.463128557318302, 'only')\n",
      "(-4.453883499174251, 'high')\n",
      "(-4.43564591162447, 'healthy')\n",
      "(-4.40013922316756, 'fresh')\n",
      "(-4.40013922316756, 'old')\n",
      "(-4.3546768490908025, 'perfect')\n",
      "(-4.321887026267812, 'regular')\n",
      "(-4.297981505414257, 'different')\n",
      "(-4.297981505414257, 'small')\n",
      "(-4.23689381343442, 'easy')\n",
      "(-4.214914906715644, 'strong')\n",
      "(-4.210095620279695, 'few')\n",
      "(-4.2005261692635445, 'sweet')\n",
      "(-4.090178112094679, 'favorite')\n",
      "(-4.036223117972268, 'first')\n",
      "(-4.0342049538160305, 'nice')\n",
      "(-4.02817478478944, 'free')\n",
      "(-4.004411290337254, 'delicious')\n",
      "(-3.98887699737507, 'many')\n",
      "(-3.973580331999596, 'hot')\n",
      "(-3.847190126590327, 'much')\n",
      "(-3.3801172090926714, 'little')\n",
      "(-3.1292687640448014, 'other')\n",
      "(-2.718921628339329, 'great')\n",
      "(-2.4471940771693745, 'good')\n"
     ]
    }
   ],
   "source": [
    "for i in adj:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that adjectives with a negative connotation are judged by our model with numbers which are more negative than those with a positive connotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
